{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1247628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "import csv\n",
    "from surprise.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "56afba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        try:\n",
    "            g = d['gameID']\n",
    "        except Exception as e:\n",
    "            g = None\n",
    "        yield u,g,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "70c686b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Would-play baseline: just rank which games are popular and which are not, \n",
    "##and return '1' if a game is among the top-ranked\n",
    "\n",
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0 \n",
    "\n",
    "for user, game, _ in readJSON(\"train.json.gz\"):\n",
    "    gameCount[game] += 1\n",
    "    totalPlayed += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6f958f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHours = []\n",
    "for l in readJSON(\"train.json.gz\"):\n",
    "    allHours.append(l)\n",
    "\n",
    "hoursTrain = allHours[:165000]\n",
    "hoursValid = allHours[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d458d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostPopular = [(gameCount[x], x ) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "90195eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamesPerUser = defaultdict(set)\n",
    "usersPerGame = defaultdict(set)\n",
    "\n",
    "for user,game,d in readJSON(\"train.json.gz\"):\n",
    "    h = d['hours_transformed']\n",
    "    gamesPerUser[user].add(game)\n",
    "    usersPerGame[game].add(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2afec9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = set()\n",
    "games = set() \n",
    "\n",
    "for user,game,d in readJSON(\"train.json.gz\"): \n",
    "    users.add(user)\n",
    "    games.add(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce73ec",
   "metadata": {},
   "source": [
    "#### Preparing a validation of 50/50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "e22237f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making validation set with 50% played , 50% unplayed random\n",
    "validation = []\n",
    "\n",
    "for u, g , _ in hoursValid: \n",
    "    validation.append((u,g,1))\n",
    "    unplayed = list(games - gamesPerUser[u])\n",
    "    if unplayed :\n",
    "        negative = random.choice(unplayed)\n",
    "        validation.append((u , negative , 0))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d8deeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(pred,validation):\n",
    "    correct_pred = 0\n",
    "    for (u, g, playedOrNot), predicted in zip(validation, predictions):\n",
    "        if predicted == playedOrNot:\n",
    "            correct_pred += 1\n",
    "    accuracy = correct_pred / len(validation)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "6fa766e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(set(s1).intersection(set(s2) ))\n",
    "    denom =len(set(s1).union(set(s2)) )\n",
    "    return numer/denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61b9fc",
   "metadata": {},
   "source": [
    "##### Best-threshold for popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "30351063",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = None\n",
    "best_acc = 0\n",
    "return_t = set()\n",
    "\n",
    "for p in range(1, 100):  # Best percentage\n",
    "    threshold = totalPlayed * (p / 100)\n",
    "    count = 0\n",
    "    \n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return_t.add(i)\n",
    "        if count > threshold:\n",
    "            break\n",
    "\n",
    "    correct_pred = 0\n",
    "    for u, g, playedOrNot in validation:\n",
    "        predicted = 1 if g in return_t else 0\n",
    "        if predicted == playedOrNot:\n",
    "            correct_pred += 1\n",
    "\n",
    "    acc = correct_pred / len(validation)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_threshold = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "301c1ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70355 66\n"
     ]
    }
   ],
   "source": [
    "print(best_acc, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0be3c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_thresh = best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a567e9",
   "metadata": {},
   "source": [
    "#### Testing on different threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "31b7b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sim(u,g,threshold):\n",
    "    similarities = []\n",
    "    for g1 in gamesPerUser[u]:\n",
    "        if g1 == g: continue\n",
    "        sim = Jaccard(usersPerGame[g1], usersPerGame[g])\n",
    "        #sim = Pearson(i, i2) # Could use alternate similarity metrics straightforwardly\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    if max(similarities,default = 0) > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ea28bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(thresh):\n",
    "    predictions = 0\n",
    "    for u, g, _ in validation:\n",
    "        if predict_sim(u,g, thresh)==True and _ != None:\n",
    "            correct_pred += 1\n",
    "        elif predict_sim(u,g, thresh)==False and _ == None:\n",
    "            correct_pred += 1\n",
    "        else:\n",
    "            pass\n",
    "    return correct_pred / len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "11681f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2thresh(thre1,thre2):\n",
    "    returnthre = set()\n",
    "    count = 0 \n",
    "    \n",
    "    for ic, i in mostPopular: \n",
    "        count += ic \n",
    "        returnthre.add(i)\n",
    "        if count > totalPlayed* thre1 : break\n",
    "            \n",
    "    predictions = []\n",
    "    for u, g, _ in validation:\n",
    "        if g in returnthre or predict_sim(u,g, thre2) == True:\n",
    "            predictions.append(True)\n",
    "        else:\n",
    "            predictions.append(False)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9049179",
   "metadata": {},
   "source": [
    "#### Finding the two best thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "0fa8914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "thresholds = [] \n",
    "predictions = []\n",
    "for i in range(62,68):\n",
    "    for j in range(2,5):\n",
    "        thre1 = i*0.01\n",
    "        thre2 = j*0.01\n",
    "        predictions = model_2thresh(thre1, thre2)\n",
    "        acc = get_acc(predictions, validation)\n",
    "        thresholds.append((thre1, thre2))\n",
    "        accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "412593e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6428, 0.7567, 0.7559, 0.6425, 0.7553, 0.75605, 0.64205, 0.75375, 0.75495, 0.64175, 0.75235, 0.7546, 0.64135, 0.7508, 0.75375, 0.6408, 0.7475, 0.75215]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "acc = max(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "f7be353e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "d6f6a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "indx = accuracies.index(acc)\n",
    "print(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "885e3d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62 0.03\n"
     ]
    }
   ],
   "source": [
    "thre1, thre2 = thresholds[indx]\n",
    "print(thre1, thre2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "a7043e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thre1 = 0.63 #more stable\n",
    "thre2 = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "f211d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "returnthre = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    returnthre.add(i)\n",
    "    if count > totalPlayed*thre1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "31fa719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Played.csv\", 'w')\n",
    "\n",
    "for l in open(\"pairs_Played.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    \n",
    "    u,g = l.strip().split(',')\n",
    "    \n",
    "    if g in returnthre or predict_sim(u, g, thre2)==True:\n",
    "        predictions.write(u + ',' + g + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + ',' + g + \",0\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "46fef161",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time-played baseline: compute averages for each user, \n",
    "##or return the global average if we've never seen the user before\n",
    "\n",
    "allHours = []\n",
    "userHours = defaultdict(list)\n",
    "\n",
    "for user,game,d in readJSON(\"train.json.gz\"):\n",
    "  h = d['hours_transformed']\n",
    "  allHours.append(h)\n",
    "  userHours[user].append(h)\n",
    "\n",
    "globalAverage = sum(allHours) / len(allHours)\n",
    "userAverage = {}\n",
    "for u in userHours:\n",
    "  userAverage[u] = sum(userHours[u]) / len(userHours[u])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a951295",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHours = []\n",
    "for l in readJSON(\"train.json.gz\"):\n",
    "    allHours.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f798b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursPerUser = defaultdict(set)\n",
    "hoursPerItem = defaultdict(set)\n",
    "\n",
    "for user, game , d in allHours:\n",
    "    hoursPerUser[user].add((game, d['hours_transformed']))\n",
    "    hoursPerItem[game].add((user, d['hours_transformed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce342306",
   "metadata": {},
   "outputs": [],
   "source": [
    "betaU = {}\n",
    "betaI = {}\n",
    "for u in hoursPerUser:\n",
    "    betaU[u] = 0\n",
    "\n",
    "for g in hoursPerItem:\n",
    "    betaI[g] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1645d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = len(hoursTrain)\n",
    "users = list(hoursPerUser.keys() )\n",
    "items = list(hoursPerItem.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c97da1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(data, alpha, betaU, betaI):\n",
    "    total_loss = 0\n",
    "    for u, i, r in data:\n",
    "        prediction = alpha + betaU[u] + betaI[i]\n",
    "        total_loss += (r - prediction) ** 2\n",
    "    return total_loss / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "9e88afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(lamb):\n",
    "    \n",
    "    for itr in range(1000):\n",
    "        # Calculate alpha\n",
    "        alpha_term = globalAverage\n",
    "        newalpha_term = 0\n",
    "        for u, i, r in hoursTrain:\n",
    "            newalpha_term +=  r['hours_transformed'] - (betaU[u] + betaI[i])\n",
    "        alpha = newalpha_term / Ntrain\n",
    "\n",
    "        # Calculate beta_u\n",
    "        for u in hoursPerUser:\n",
    "            beta_u_term = 0.0\n",
    "            for i, r in hoursPerUser[u]:\n",
    "                beta_u_term += r - (alpha+ betaI[i])\n",
    "            betaU[u] = beta_u_term / (lamb + len(hoursPerUser[u]) )\n",
    "\n",
    "        # Calculate beta_i\n",
    "        for i in hoursPerItem:\n",
    "            beta_i_term = 0.0\n",
    "            for u, r in hoursPerItem[i]:\n",
    "                beta_i_term += r - (alpha+ betaU[u])\n",
    "            betaI[i] = beta_i_term / (lamb + len(hoursPerItem[i]) )\n",
    "        \n",
    "        current_loss = 0.0\n",
    "        for u, i, r in hoursTrain:\n",
    "            current_loss += (r['hours_transformed'] - (alpha + betaU[u] + betaI[i]) )**2\n",
    "        \n",
    "        regularizer = 0\n",
    "        for u in betaU :\n",
    "            regularizer += betaU[u]**2\n",
    "        for i in betaI :\n",
    "            regularizer += betaI[i]**2\n",
    "\n",
    "        current_loss /= len(hoursTrain)\n",
    "\n",
    "    return current_loss, current_loss + lamb*regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "2d6519a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse,objective = iterate(5)\n",
    "newMSE,newObjective = iterate(5)\n",
    "iterations = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a16512",
   "metadata": {},
   "source": [
    "#### Using best lambda which is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "065ec9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "while iterations < 7 or objective - newObjective > 0.00001:\n",
    "    mse, objective = newMSE, newObjective\n",
    "    newMSE, newObjective = iterate(5)\n",
    "    iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511f956",
   "metadata": {},
   "source": [
    "#### SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "fd780f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(user, item, rating) for user, item, rating in validation]\n",
    "\n",
    "reader = Reader(line_format='user item rating', sep='\\t')\n",
    "dataset = Dataset.load_from_df(pd.DataFrame(data, columns=['user', 'item', 'rating']), reader)\n",
    "\n",
    "trainset, valset = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "674cc013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5122\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_epochs': [5, 10, 15, 20, 25], 'lr_all': [0.002, 0.005, 0.01, 0.1],\n",
    "              'reg_all': [0.02, 0.1, 0.2, 0.5], 'n_factors': [1, 2, 3]}\n",
    "\n",
    "svd = SVD()\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "grid_search.fit(dataset)  # Use the original dataset, not trainset\n",
    "best_params = grid_search.best_params['rmse']\n",
    "final_model = SVD(**best_params)\n",
    "final_model.fit(trainset)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = final_model.test(valset)\n",
    "val_mse = accuracy.mse(val_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "27328b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_epochs': 5, 'lr_all': 0.002, 'reg_all': 0.02, 'n_factors': 1}"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "3407cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVD(n_factors=2,n_epochs=5,lr_all=0.002,reg_pu=1e-3,reg_qi=1e-3)\n",
    "model.fit(trainset)\n",
    "predictions = model.test(testset)\n",
    "sse = 0\n",
    "for p in predictions:\n",
    "    sse += (p.r_ui - p.est)**2\n",
    "\n",
    "see=(sse / len(predictions))\n",
    "see\n",
    "final_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "bf395dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open('predictions_Hours.csv', 'w')\n",
    "for l in open(\"pairs_hours.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    bu = 0\n",
    "    bi = 0\n",
    "    if u in betaU:\n",
    "        bu = betaU[u]\n",
    "    if g in betaI:\n",
    "        bi = betaI[g]\n",
    "        \n",
    "    try:\n",
    "        inner_uid = trainset.to_inner_uid(u)\n",
    "        latent_u = final_model.pu[inner_uid]\n",
    "    except ValueError:  # user u not in trainset\n",
    "        latent_u = np.zeros(final_model.n_factors)\n",
    "\n",
    "    try:\n",
    "        inner_iid = trainset.to_inner_iid(g)\n",
    "        latent_i = final_model.qi[inner_iid]\n",
    "    except ValueError:  # item g not in trainset\n",
    "        latent_i = np.zeros(final_model.n_factors)\n",
    "       \n",
    "    _ = predictions.write(u + ',' + g + ',' + str(alpha + bu + bi+ np.dot(latent_u, latent_i)) + '\\n')\n",
    "  \n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8eb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
